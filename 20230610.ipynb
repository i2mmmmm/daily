{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjIfznJBtttEPVjpJYCfmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i2mmmmm/daily/blob/main/20230610.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN : Recurrent Neural Network 순환 신경망\n",
        "\n",
        "연속 데이터(sequential data) 처리하는 인공 신경망의 일종\n",
        "\n",
        "연속된 입력에 대해 이전 상태의 정보를 기억하고 활용하는 특징\n",
        "\n",
        "이를 통해 문장의 의미나 문맥을 이해하고, 예측하거나 분류하는 작업을 수행\n",
        "\n",
        "**▶은닉층 (hidden state)이라는 이전 단계에서 계산된 정보를 저장하여 다음 단계에서 활용 = 은닉 벡터가 입력 벡터가 되어 다시 들어가기에 순환 신경망 이라고 한다.**\n",
        "\n",
        "RNN의 변형 중 하나인 LSTM(Long Short-Term Memory) GRU(Gated Recurrent Unit)이 시계열 데이터 예측에 더 효과적\n",
        "\n",
        "- 이전 단계가 끝나야 다음 단계가 진행될 수 있는 특징 때문에 GPU의 장점인 병렬화가 불가능하다.\n",
        "- 기울기 폭발, 기울기 소실이 발생할 수 있다. ->LSTM으로 해결\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "### LSTM 장단기 기억망\n",
        "\n",
        "RNN 기울기 소실을 해결하려고 3가지 게이트가 더 추가됨\n",
        "forget gate, input gate, output gate\n",
        "\n",
        "-> 언어 모델, 시계열 알고리즘에서 사용\n",
        "\n",
        "-----------------------------------------------------------------------\n",
        "### GRU는 LSTM의 간소화 버전"
      ],
      "metadata": {
        "id": "gcsp4yElpfn7"
      }
    }
  ]
}